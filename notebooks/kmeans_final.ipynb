{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import hstack, csr_matrix, issparse\n",
    "\n",
    "\n",
    "PATH_PRODUCTS = \"../data/raw/products_with_prices_ingredients_nutrition.csv\"  # enriched file\n",
    "PATH_AISLES = \"../data/raw/aisles.csv\"\n",
    "PATH_DEPTS = \"../data/raw/departments.csv\"\n",
    "\n",
    "PATH_EXISTING_BUCKETS = None  # optional\n",
    "OUT_DIR = \"../data/kmeans_artifacts\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "SIZE_PAT = re.compile(\n",
    "    r\"(?:\\b\\d+(?:\\.\\d+)?\\s?(?:oz|ounce|ounces|lb|lbs|pound|g|gram|grams|kg|ml|l|liter|liters|ct|count|pack|pk)\\b)\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "\n",
    "def clean_name(s):\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(SIZE_PAT, \" \", s)  # remove size tokens\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\b\\d+\\b\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def extract_value_per_100g(row):\n",
    "    price = row.get(\"Price_USD\", np.nan)\n",
    "    name = str(row.get(\"product_name\", \"\"))\n",
    "    sizes = SIZE_PAT.findall(name)\n",
    "    mass_ml = None\n",
    "    for token in sizes:\n",
    "        t = token.lower().replace(\"ounces\", \"oz\").replace(\"ounce\", \"oz\")\n",
    "        m = re.findall(r\"(\\d+(?:\\.\\d+)?)\\s?(oz|lb|g|kg|ml|l)\", t)\n",
    "        if m:\n",
    "            val, unit = m[0]\n",
    "            val = float(val)\n",
    "            if unit == \"oz\":\n",
    "                mass_ml = val * 28.3495\n",
    "            elif unit == \"lb\":\n",
    "                mass_ml = val * 453.592\n",
    "            elif unit == \"g\":\n",
    "                mass_ml = val\n",
    "            elif unit == \"kg\":\n",
    "                mass_ml = val * 1000.0\n",
    "            elif unit == \"ml\":\n",
    "                mass_ml = val\n",
    "            elif unit == \"l\":\n",
    "                mass_ml = val * 1000.0\n",
    "            break\n",
    "    if pd.notnull(price) and mass_ml and mass_ml > 0:\n",
    "        return price / (mass_ml / 100.0)\n",
    "    return price\n",
    "\n",
    "\n",
    "def quick_make_buckets(df, name_col=\"product_name\", price_col=\"Price_USD\", price_tol=0.12):\n",
    "    base = df.copy()\n",
    "    base[\"norm_name\"] = base[name_col].fillna(\"\").str.lower()\n",
    "    base[\"norm_name\"] = base[\"norm_name\"].str.replace(r\"[^a-z0-9\\s]\", \" \", regex=True)\n",
    "    base[\"norm_name_nosize\"] = (\n",
    "        base[\"norm_name\"].str.replace(SIZE_PAT, \" \", regex=True).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    )\n",
    "\n",
    "    bucket_ids = np.empty(len(base), dtype=object)\n",
    "    next_id = 1\n",
    "    for nn, sub in base.groupby(\"norm_name_nosize\"):\n",
    "        if nn == \"\":\n",
    "            for idx in sub.index:\n",
    "                bucket_ids[idx] = f\"B{next_id:07d}\"\n",
    "                next_id += 1\n",
    "            continue\n",
    "        prices = sub[price_col].astype(float).fillna(sub[price_col].astype(float).median())\n",
    "        med = prices.median()\n",
    "        band_mask = (prices >= (1 - price_tol) * med) & (prices <= (1 + price_tol) * med)\n",
    "        band_ids = []\n",
    "        for idx in sub.index:\n",
    "            band_ids.append(\"A\" if band_mask.loc[idx] else \"B\")\n",
    "        for idx, b in zip(sub.index, band_ids):\n",
    "            bucket_ids[idx] = f\"B{next_id:07d}_{b}\"\n",
    "            next_id += 1\n",
    "\n",
    "    return bucket_ids\n",
    "\n",
    "\n",
    "prod = pd.read_csv(PATH_PRODUCTS)\n",
    "aisl = pd.read_csv(PATH_AISLES)\n",
    "dept = pd.read_csv(PATH_DEPTS)\n",
    "\n",
    "prod.columns = [c.strip() for c in prod.columns]\n",
    "aisl.columns = [c.strip() for c in aisl.columns]\n",
    "dept.columns = [c.strip() for c in dept.columns]\n",
    "\n",
    "df = prod.merge(aisl, on=\"aisle_id\", how=\"left\").merge(dept, on=\"department_id\", how=\"left\")\n",
    "\n",
    "\n",
    "df[\"value_per_100g\"] = df.apply(lambda r: extract_value_per_100g(r), axis=1)\n",
    "\n",
    "\n",
    "if \"Price_USD\" in df.columns:\n",
    "    med_price = pd.to_numeric(df[\"Price_USD\"], errors=\"coerce\").median()\n",
    "    df[\"Price_USD\"] = pd.to_numeric(df[\"Price_USD\"], errors=\"coerce\").fillna(med_price)\n",
    "else:\n",
    "    df[\"Price_USD\"] = pd.to_numeric(df[\"value_per_100g\"], errors=\"coerce\").fillna(\n",
    "        pd.to_numeric(df[\"value_per_100g\"], errors=\"coerce\").median()\n",
    "    )\n",
    "\n",
    "if PATH_EXISTING_BUCKETS and os.path.exists(PATH_EXISTING_BUCKETS):\n",
    "    p1 = pd.read_csv(PATH_EXISTING_BUCKETS)\n",
    "    df = df.merge(p1[[\"product_id\", \"p1_bucket_id\"]], on=\"product_id\", how=\"left\")\n",
    "else:\n",
    "    df[\"p1_bucket_id\"] = quick_make_buckets(df)\n",
    "\n",
    "\n",
    "df[\"name_clean\"] = df[\"product_name\"].map(clean_name).fillna(\"\")\n",
    "text_col = \"name_clean\"\n",
    "\n",
    "df[\"aisle\"] = df[\"aisle\"].fillna(\"unknown_aisle\")\n",
    "df[\"department\"] = df[\"department\"].fillna(\"unknown_department\")\n",
    "\n",
    "\n",
    "nan_counts = {c: int(df[c].isna().sum()) for c in df.columns if df[c].isna().sum() > 0}\n",
    "if nan_counts:\n",
    "    print(\"[WARN] Columns with NaNs before feature engineering:\", nan_counts)\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=5, max_df=0.7, ngram_range=(1, 2))\n",
    "svd = TruncatedSVD(n_components=64, random_state=42)\n",
    "text_pipe = make_pipeline(tfidf, svd)\n",
    "X_text = text_pipe.fit_transform(df[text_col])  # TruncatedSVD returns dense array\n",
    "\n",
    "\n",
    "if np.isnan(X_text).any():\n",
    "    print(\"[WARN] NaNs in X_text detected; replacing with 0s.\")\n",
    "    X_text = np.nan_to_num(X_text, nan=0.0)\n",
    "\n",
    "\n",
    "if not issparse(X_text):\n",
    "    X_text = csr_matrix(X_text)\n",
    "\n",
    "\n",
    "num_feats = df[[\"Price_USD\", \"value_per_100g\"]].astype(float)\n",
    "num_feats = num_feats.fillna(num_feats.median())  # column-wise median fallback\n",
    "num_scaler = RobustScaler()\n",
    "X_num = num_scaler.fit_transform(num_feats)  # dense\n",
    "if np.isnan(X_num).any():\n",
    "    print(\"[WARN] NaNs in X_num after scaling; replacing with 0s.\")\n",
    "    X_num = np.nan_to_num(X_num, nan=0.0)\n",
    "X_num = csr_matrix(X_num)  # sparse\n",
    "\n",
    "\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "X_aisle = ohe.fit_transform(df[[\"aisle\"]].astype(str))\n",
    "\n",
    "if hasattr(X_aisle, \"toarray\"):\n",
    "    if np.isnan(X_aisle.toarray()).any():\n",
    "        print(\"[WARN] NaNs in X_aisle detected; converting to 0s.\")\n",
    "        X_aisle = csr_matrix(np.nan_to_num(X_aisle.toarray(), nan=0.0))\n",
    "\n",
    "\n",
    "X = hstack([X_text, X_num, X_aisle]).tocsr()\n",
    "\n",
    "if np.isnan(X.data).any():\n",
    "    print(\"[WARN] NaNs found in stacked feature matrix; replacing with 0s.\")\n",
    "    X.data = np.nan_to_num(X.data, nan=0.0)\n",
    "\n",
    "sample_check_n = min(5000, X.shape[0])\n",
    "sample_block = X[:sample_check_n].toarray()\n",
    "if np.isnan(sample_block).any():\n",
    "\n",
    "    print(\"[ERROR] NaNs still present in sample of stacked matrix — reporting column sources.\")\n",
    "\n",
    "    def block_has_nans(mat, name):\n",
    "        if issparse(mat):\n",
    "            arr = mat[:sample_check_n].toarray()\n",
    "        else:\n",
    "            arr = np.asarray(mat)[:sample_check_n]\n",
    "        return np.isnan(arr).any()\n",
    "\n",
    "    print(\" X_text NaN?\", block_has_nans(X_text, \"X_text\"))\n",
    "    print(\" X_num NaN?\", block_has_nans(X_num, \"X_num\"))\n",
    "    print(\" X_aisle NaN?\", block_has_nans(X_aisle, \"X_aisle\"))\n",
    "    raise ValueError(\"Feature matrix contains NaNs after preprocessing. Inspect inputs.\")\n",
    "print(f\"[OK] Feature matrix built: shape={X.shape}, sample NaN check passed.\")\n",
    "\n",
    "\n",
    "tfidf_params = {\n",
    "    \"min_df\": int(tfidf.min_df),\n",
    "    \"max_df\": float(tfidf.max_df) if tfidf.max_df is not None else None,\n",
    "    \"ngram_range\": list(tfidf.ngram_range),\n",
    "    \"vocab_size\": int(len(tfidf.vocabulary_)) if getattr(tfidf, \"vocabulary_\", None) is not None else None,\n",
    "}\n",
    "with open(os.path.join(OUT_DIR, \"kmeans_feature_meta.json\"), \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"tfidf_params\": tfidf_params,\n",
    "            \"svd_n_components\": int(svd.n_components),\n",
    "            \"numeric_cols\": [\"Price_USD\", \"value_per_100g\"],\n",
    "            \"ohe_aisle_ncats\": int(X_aisle.shape[1]),\n",
    "        },\n",
    "        f,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "\n",
    "def sweep_kmeans_metrics(X, k_list, random_state=42, sample_for_scores=30000, densify_max=8000):\n",
    "    n = X.shape[0]\n",
    "    if n > sample_for_scores:\n",
    "        rng = np.random.default_rng(random_state)\n",
    "        idx = rng.choice(n, size=sample_for_scores, replace=False)\n",
    "        Xs = X[idx]\n",
    "    else:\n",
    "        Xs = X\n",
    "\n",
    "    results = []\n",
    "    for k in k_list:\n",
    "        km = KMeans(n_clusters=k, init=\"k-means++\", n_init=10, random_state=random_state)\n",
    "        km.fit(Xs)\n",
    "        labels = km.labels_\n",
    "        inertia = float(km.inertia_)\n",
    "\n",
    "        sil = silhouette_score(Xs, labels, metric=\"cosine\")\n",
    "\n",
    "        if Xs.shape[0] > densify_max:\n",
    "            rng = np.random.default_rng(random_state)\n",
    "            sub_idx = rng.choice(Xs.shape[0], size=densify_max, replace=False)\n",
    "            X_for_metrics = Xs[sub_idx]\n",
    "            labels_for_metrics = labels[sub_idx]\n",
    "        else:\n",
    "            X_for_metrics = Xs\n",
    "            labels_for_metrics = labels\n",
    "\n",
    "        X_dense = X_for_metrics.toarray() if hasattr(X_for_metrics, \"toarray\") else np.asarray(X_for_metrics)\n",
    "        dbi = davies_bouldin_score(X_dense, labels_for_metrics)\n",
    "        ch = calinski_harabasz_score(X_dense, labels_for_metrics)\n",
    "        results.append({\"k\": int(k), \"inertia\": inertia, \"silhouette\": float(sil) , \"dbi\": float(dbi), \"calinski_h\": float(ch)})\n",
    "        print(f\"[k={k}] inertia={inertia:.2f} silhouette={sil:.4f} dbi={dbi:.4f} calinski={ch:.1f}\")\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "k_list = [24, 32, 40, 56, 64, 80, 96, 112, 128 , 150]\n",
    "metrics_df = sweep_kmeans_metrics(X, k_list)\n",
    "metrics_df.to_csv(os.path.join(OUT_DIR, \"kmeans_metrics_sweep.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].plot(metrics_df[\"k\"], metrics_df[\"inertia\"], marker=\"o\")\n",
    "ax[0].set_title(\"Elbow: Inertia vs K\")\n",
    "ax[0].set_xlabel(\"K\")\n",
    "ax[0].set_ylabel(\"Inertia\")\n",
    "\n",
    "ax[1].plot(metrics_df[\"k\"], metrics_df[\"silhouette\"], marker=\"o\", label=\"Silhouette (↑)\")\n",
    "ax[1].plot(metrics_df[\"k\"], metrics_df[\"dbi\"], marker=\"s\", label=\"Davies–Bouldin (↓)\")\n",
    "ax[1].plot(metrics_df[\"k\"], metrics_df[\"calinski_h\"], marker=\"^\", label=\"Calinski–Harabasz (↑)\")\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"Internal Metrics vs K\")\n",
    "ax[1].set_xlabel(\"K\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"kmeans_elbow_internal_metrics.png\"), dpi=180)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "BEST_K = int(metrics_df.sort_values([\"silhouette\", \"calinski_h\"], ascending=[False, False]).iloc[0][\"k\"])\n",
    "print(f\"Chosen K (heuristic): {BEST_K}\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=BEST_K, init=\"k-means++\", n_init=10, random_state=42)\n",
    "labels_full = kmeans.fit_predict(X)\n",
    "df[\"p2_cluster_id\"] = labels_full\n",
    "df.to_csv(os.path.join(OUT_DIR, \"products_with_kmeans_labels.csv\"), index=False)\n",
    "\n",
    "\n",
    "def build_kpartite_graph(df, X, max_p1_per_cluster=30):\n",
    "    G = nx.Graph()\n",
    "    aisles = df[\"aisle\"].fillna(\"unknown\").unique().tolist()\n",
    "    for a in aisles:\n",
    "        G.add_node((\"P3\", a), layer=\"P3\", label=a)\n",
    "\n",
    "    clusters = sorted(df[\"p2_cluster_id\"].unique().tolist())\n",
    "    for c in clusters:\n",
    "        G.add_node((\"P2\", int(c)), layer=\"P2\", label=f\"C{int(c)}\")\n",
    "\n",
    "    for (c, a), sub in df.groupby([\"p2_cluster_id\", \"aisle\"]):\n",
    "        subs = sub.copy()\n",
    "        if subs.shape[0] > max_p1_per_cluster:\n",
    "            subs = subs.sample(max_p1_per_cluster, random_state=42)\n",
    "        for _, row in subs.iterrows():\n",
    "            p1 = row[\"p1_bucket_id\"]\n",
    "            if not G.has_node((\"P1\", p1)):\n",
    "                G.add_node((\"P1\", p1), layer=\"P1\", label=str(p1))\n",
    "            G.add_edge((\"P1\", p1), (\"P2\", int(c)))\n",
    "            G.add_edge((\"P2\", int(c)), (\"P3\", row[\"aisle\"] if pd.notnull(row[\"aisle\"]) else \"unknown\"))\n",
    "\n",
    "    centroids = []\n",
    "    cluster_list = clusters\n",
    "    for c in cluster_list:\n",
    "        idx = np.where(df[\"p2_cluster_id\"].values == c)[0]\n",
    "        if len(idx) > 2000:\n",
    "            idx = np.random.default_rng(42).choice(idx, size=2000, replace=False)\n",
    "        Xi = X[idx]\n",
    "        cen = np.asarray(Xi.mean(axis=0)).reshape(-1)\n",
    "        centroids.append(cen)\n",
    "\n",
    "    C = np.vstack(centroids)\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(6, C.shape[0]), metric=\"cosine\").fit(C)\n",
    "    distances, indices = nbrs.kneighbors(C)\n",
    "    for i, (drow, irow) in enumerate(zip(distances, indices)):\n",
    "        c_i = cluster_list[i]\n",
    "        for dist, j in zip(drow[1:], irow[1:]):\n",
    "            c_j = cluster_list[j]\n",
    "            sim = 1.0 - float(dist)\n",
    "            if sim >= 0.75:\n",
    "                G.add_edge((\"P2\", int(c_i)), (\"P2\", int(c_j)), weight=sim)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "G = build_kpartite_graph(df, X)\n",
    "\n",
    "\n",
    "def draw_kpartite(G, title=\"K-Partite Product Graph (KMeans)\", out_path=None, sample_p1=800):\n",
    "    p1_nodes = [n for n, d in G.nodes(data=True) if d.get(\"layer\") == \"P1\"]\n",
    "    if len(p1_nodes) > sample_p1:\n",
    "        keep_idx = np.random.default_rng(1).choice(len(p1_nodes), size=sample_p1, replace=False)\n",
    "        p1_keep = set([p1_nodes[i] for i in keep_idx])\n",
    "        H = G.copy()\n",
    "        for n in p1_nodes:\n",
    "            if n not in p1_keep:\n",
    "                H.remove_node(n)\n",
    "    else:\n",
    "        H = G\n",
    "\n",
    "    layers = {\"P1\": 0, \"P2\": 1, \"P3\": 2}\n",
    "    pos = {}\n",
    "    for n, d in H.nodes(data=True):\n",
    "        layer = d[\"layer\"]\n",
    "        pos[n] = (layers.get(layer, 1), np.random.random())\n",
    "\n",
    "    color_map = {\"P1\": \"#99d8c9\", \"P2\": \"#2ca25f\", \"P3\": \"#e5f5f9\"}\n",
    "    sizes = {\"P1\": 40, \"P2\": 250, \"P3\": 450}\n",
    "\n",
    "    node_colors = [color_map[H.nodes[n][\"layer\"]] for n in H.nodes()]\n",
    "    node_sizes = [sizes[H.nodes[n][\"layer\"]] for n in H.nodes()]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    nx.draw_networkx_edges(H, pos, alpha=0.25, width=0.6)\n",
    "    nx.draw_networkx_nodes(H, pos, node_color=node_colors, node_size=node_sizes, linewidths=0.5, edgecolors=\"#444\")\n",
    "    labels = {n: d[\"label\"] for n, d in H.nodes(data=True) if d[\"layer\"] in (\"P2\", \"P3\")}\n",
    "    nx.draw_networkx_labels(H, pos, labels=labels, font_size=8)\n",
    "\n",
    "    import matplotlib.patches as mpatches\n",
    "\n",
    "    patches = [\n",
    "        mpatches.Patch(color=color_map[\"P1\"], label=\"P1: Buckets\"),\n",
    "        mpatches.Patch(color=color_map[\"P2\"], label=\"P2: KMeans Clusters\"),\n",
    "        mpatches.Patch(color=color_map[\"P3\"], label=\"P3: Aisles\"),\n",
    "    ]\n",
    "    plt.legend(handles=patches, loc=\"lower right\")\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    if out_path:\n",
    "        plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "draw_kpartite(G, title=f\"K-Partite Product Graph (KMeans, K={BEST_K})\", out_path=os.path.join(OUT_DIR, \"kpartite_kmeans.png\"))\n",
    "\n",
    "\n",
    "def draw_single_cluster(G, cluster_id, out_path=None):\n",
    "    nodes_p2 = [(\"P2\", int(cluster_id))]\n",
    "    nbrs = set()\n",
    "    for n in nodes_p2:\n",
    "        nbrs.update(list(G.neighbors(n)))\n",
    "    sub_nodes = set(nodes_p2) | nbrs\n",
    "    H = G.subgraph(sub_nodes).copy()\n",
    "\n",
    "    pos = nx.spring_layout(H, seed=42, k=0.45)\n",
    "    color_map = {\"P1\": \"#99d8c9\", \"P2\": \"#2ca25f\", \"P3\": \"#e5f5f9\"}\n",
    "    sizes = {\"P1\": 60, \"P2\": 400, \"P3\": 450}\n",
    "    node_colors = [color_map[H.nodes[n][\"layer\"]] for n in H.nodes()]\n",
    "    node_sizes = [sizes[H.nodes[n][\"layer\"]] for n in H.nodes()]\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    nx.draw_networkx_edges(H, pos, alpha=0.3, width=0.8)\n",
    "    nx.draw_networkx_nodes(H, pos, node_color=node_colors, node_size=node_sizes, edgecolors=\"#333\", linewidths=0.6)\n",
    "    labels = {n: H.nodes[n][\"label\"] for n in H.nodes() if H.nodes[n][\"layer\"] in (\"P2\", \"P3\")}\n",
    "    nx.draw_networkx_labels(H, pos, labels=labels, font_size=9)\n",
    "    plt.title(f\"Cluster {cluster_id}: Neighborhood\")\n",
    "    plt.axis(\"off\")\n",
    "    if out_path:\n",
    "        plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "top_cluster = int(df[\"p2_cluster_id\"].value_counts().index[0])\n",
    "draw_single_cluster(G, top_cluster, out_path=os.path.join(OUT_DIR, f\"cluster_{top_cluster}_neighborhood.png\"))\n",
    "\n",
    "print(f\"[DONE] Artifacts at {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "# --- Colors ---\n",
    "C_P3 = '#e5f5f9'    # Aisle (P3)\n",
    "C_P2 = '#2ca25f'    # KMeans cluster (P2)\n",
    "C_BG = \"#ffffff\"\n",
    "\n",
    "# Edges\n",
    "C_EdgeP2P3 = '#696969'   # Dim gray\n",
    "C_EdgeP2P2 = '#ff7f0e'   # Vibrant orange\n",
    "\n",
    "\n",
    "def draw_p2p3_graph_enhanced_kmeans(G, title=\"Enhanced KMeans Substitution Graph\"):\n",
    "\n",
    "    # ---- CLEAN, EXTENDED LAYOUT ----\n",
    "    pos = nx.spring_layout(\n",
    "    G,\n",
    "    seed=42,\n",
    "    k=1.8,          # ⬅️ MUCH larger repulsion → BIGGER spacing\n",
    "    iterations=120, # ⬅️ more stable spread\n",
    "    weight='weight' # use similarity weights correctly\n",
    ")\n",
    "\n",
    "    # ---- Node groups ----\n",
    "    p2_nodes = [n for n, d in G.nodes(data=True) if d.get(\"layer\") == \"P2\"]\n",
    "    p3_nodes = [n for n, d in G.nodes(data=True) if d.get(\"layer\") == \"P3\"]\n",
    "\n",
    "    # ---- Sizes ----\n",
    "    p2_sizes = [700] * len(p2_nodes)\n",
    "    p3_sizes = [320] * len(p3_nodes)\n",
    "\n",
    "    # ---- Edge categories (corrected logic) ----\n",
    "    e_p2p2 = [(u, v) for u, v, d in G.edges(data=True) if d.get(\"etype\") == \"P2-P2\"]\n",
    "    e_p2p3 = [(u, v) for u, v, d in G.edges(data=True) if d.get(\"etype\") == \"P2-P3\"]\n",
    "\n",
    "    p2p2_widths = [\n",
    "        max(2, d.get(\"weight\", 0.5) * 5)\n",
    "        for u, v, d in G.edges(data=True) if d.get(\"etype\") == \"P2-P2\"\n",
    "    ]\n",
    "    p2p3_widths = [2.8] * len(e_p2p3)\n",
    "\n",
    "    # ---- Figure ----\n",
    "    fig, ax = plt.subplots(figsize=(26, 20), dpi=160)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_facecolor(C_BG)\n",
    "    plt.title(title, fontsize=24, weight='bold')\n",
    "\n",
    "    # Helper\n",
    "    def segs(edge_list):\n",
    "        return [(pos[u], pos[v]) for u, v in edge_list]\n",
    "\n",
    "    # ---- Draw edges ----\n",
    "    lc_p2p3 = LineCollection(\n",
    "        segs(e_p2p3), colors=C_EdgeP2P3, linewidths=p2p3_widths, alpha=0.9\n",
    "    )\n",
    "    ax.add_collection(lc_p2p3)\n",
    "    lc_p2p3.set_zorder(1)\n",
    "\n",
    "    lc_p2p2 = LineCollection(\n",
    "        segs(e_p2p2), colors=C_EdgeP2P2, linewidths=p2p2_widths, alpha=0.75\n",
    "    )\n",
    "    ax.add_collection(lc_p2p2)\n",
    "    lc_p2p2.set_zorder(1.1)\n",
    "\n",
    "    # ---- Draw nodes ----\n",
    "    p3_coll = nx.draw_networkx_nodes(\n",
    "        G, pos, nodelist=p3_nodes, node_color=C_P3, node_size=p3_sizes,\n",
    "        edgecolors=\"#4a4a4a\", linewidths=0.8, alpha=0.95, ax=ax\n",
    "    )\n",
    "    p3_coll.set_zorder(2)\n",
    "\n",
    "    p2_coll = nx.draw_networkx_nodes(\n",
    "        G, pos, nodelist=p2_nodes, node_color=C_P2, node_size=p2_sizes,\n",
    "        edgecolors=\"#225e4d\", linewidths=1.4, alpha=0.98, ax=ax\n",
    "    )\n",
    "    p2_coll.set_zorder(3)\n",
    "\n",
    "    # ---- Labels ----\n",
    "    p2_labels = {n: d[\"label\"] for n, d in G.nodes(data=True) if d.get(\"layer\") == \"P2\"}\n",
    "    p2_texts = nx.draw_networkx_labels(\n",
    "        G, pos, labels=p2_labels, font_size=13, font_weight='bold',\n",
    "        font_color='#e5f5f9', ax=ax\n",
    "    )\n",
    "    for t in p2_texts.values(): t.set_zorder(4)\n",
    "\n",
    "    p3_labels = {n: d[\"label\"] for n, d in G.nodes(data=True) if d.get(\"layer\") == \"P3\"}\n",
    "    p3_texts = nx.draw_networkx_labels(\n",
    "        G, pos, labels=p3_labels, font_size=11,\n",
    "        font_color='#073642', ax=ax\n",
    "    )\n",
    "    for t in p3_texts.values(): t.set_zorder(3)\n",
    "\n",
    "\n",
    "    legend_elems = [\n",
    "        Line2D([0], [0], marker='o', color='w',\n",
    "               label=\"P2: KMeans Cluster\",\n",
    "               markerfacecolor=C_P2, markeredgecolor='#225e4d', markersize=12),\n",
    "        Line2D([0], [0], marker='o', color='w',\n",
    "               label=\"P3: Aisle\",\n",
    "               markerfacecolor=C_P3, markeredgecolor='#4a4a4a', markersize=10),\n",
    "        Line2D([0], [0], color=C_EdgeP2P2, lw=6,\n",
    "               label=\"P2–P2 Similarity (Orange)\"),\n",
    "        Line2D([0], [0], color=C_EdgeP2P3, lw=3,\n",
    "               label=\"P2–P3 Membership (Gray)\"),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elems, loc=\"upper left\", fontsize=12, title=\"Graph Elements\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    print(\"Enhanced P2–P3 visualization completed (decluttered).\")\n",
    "\n",
    "\n",
    "\n",
    "draw_p2p3_graph_enhanced_kmeans(G, title=f\"KMeans Enhanced Substitution Graph (K={BEST_K})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal K by criterion:\n",
      " BIC: 24\n",
      " Silhouette: 112\n",
      " Davies-Bouldin: 150\n",
      "[OK] Model selection dashboard saved to ../data/kmeans_artifacts/kmeans_model_selection_dashboard.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def compute_kmeans_bic_aic(X, k_list, metrics_df):\n",
    "\n",
    "    n, d = X.shape\n",
    "    bic_scores = []\n",
    "    aic_scores = []\n",
    "\n",
    "    for _, row in metrics_df.iterrows():\n",
    "        k = int(row['k'])\n",
    "        inertia = row['inertia']\n",
    "\n",
    "        # Avoid log(0)\n",
    "        log_likelihood = -n * np.log(max(inertia / n, 1e-10))\n",
    "\n",
    "        bic = -2 * log_likelihood + k * np.log(n) * d\n",
    "        aic = -2 * log_likelihood + 2 * k * d\n",
    "\n",
    "        bic_scores.append(bic)\n",
    "        aic_scores.append(aic)\n",
    "\n",
    "    metrics_df['bic'] = bic_scores\n",
    "    metrics_df['aic'] = aic_scores\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "\n",
    "metrics_df = compute_kmeans_bic_aic(X, k_list, metrics_df)\n",
    "\n",
    "\n",
    "optimal_bic = metrics_df.loc[metrics_df['bic'].idxmin(), 'k']\n",
    "optimal_silhouette = metrics_df.loc[metrics_df['silhouette'].idxmax(), 'k']\n",
    "optimal_dbi = metrics_df.loc[metrics_df['dbi'].idxmin(), 'k']\n",
    "\n",
    "print(\"\\nOptimal K by criterion:\")\n",
    "print(f\" BIC: {int(optimal_bic)}\")\n",
    "print(f\" Silhouette: {int(optimal_silhouette)}\")\n",
    "print(f\" Davies-Bouldin: {int(optimal_dbi)}\")\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.plot(metrics_df['k'], metrics_df['bic'], 'o-', color='#1f77b4',\n",
    "         linewidth=2, markersize=8, label='BIC (Lower is Better)')\n",
    "ax1.plot(metrics_df['k'], metrics_df['aic'], 'o-', color='#ff7f0e',\n",
    "         linewidth=2, markersize=8, label='AIC (Lower is Better)')\n",
    "\n",
    "\n",
    "ax1.set_xlabel('K', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('KMeans: BIC/AIC Curve', fontsize=14, fontweight='bold', pad=15)\n",
    "ax1.legend(loc='best', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "ax1.tick_params(labelsize=10)\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.plot(metrics_df['k'], metrics_df['silhouette'], 'o-', color='#1f77b4',\n",
    "         linewidth=2, markersize=8)\n",
    "\n",
    "\n",
    "ax2.set_xlabel('K', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('KMeans: Silhouette Score', fontsize=14, fontweight='bold', pad=15)\n",
    "ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "ax2.tick_params(labelsize=10)\n",
    "\n",
    "\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax3.plot(metrics_df['k'], metrics_df['dbi'], 'o-', color='#1f77b4',\n",
    "         linewidth=2, markersize=8)\n",
    "\n",
    "\n",
    "\n",
    "ax3.set_xlabel('K', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('KMeans: Davies-Bouldin (Lower is Better)',\n",
    "              fontsize=14, fontweight='bold', pad=15)\n",
    "ax3.grid(True, alpha=0.3, linestyle='--')\n",
    "ax3.tick_params(labelsize=10)\n",
    "\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "max_samples_pca = 5000\n",
    "if X.shape[0] > max_samples_pca:\n",
    "    sample_idx = np.random.choice(X.shape[0], size=max_samples_pca, replace=False)\n",
    "    X_pca_sample = X[sample_idx]\n",
    "    labels_pca_sample = labels_full[sample_idx]\n",
    "else:\n",
    "    X_pca_sample = X\n",
    "    labels_pca_sample = labels_full\n",
    "\n",
    "X_dense_pca = X_pca_sample.toarray() if hasattr(X_pca_sample, 'toarray') else np.asarray(X_pca_sample)\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca_2d = pca.fit_transform(X_dense_pca)\n",
    "\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(np.unique(labels_pca_sample))))\n",
    "\n",
    "for cluster_id in np.unique(labels_pca_sample):\n",
    "    mask = labels_pca_sample == cluster_id\n",
    "    ax4.scatter(\n",
    "        X_pca_2d[mask, 0],\n",
    "        X_pca_2d[mask, 1],\n",
    "        c=[colors[cluster_id % len(colors)]],\n",
    "        alpha=0.6,\n",
    "        s=20,\n",
    "        edgecolors='none',\n",
    "        label=f\"C{cluster_id}\" if cluster_id < 10 else None\n",
    "    )\n",
    "\n",
    "ax4.set_xlabel('PC1', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('PC2', fontsize=12, fontweight='bold')\n",
    "ax4.set_title(f'KMeans PCA 2D View (K={BEST_K})', fontsize=14,\n",
    "              fontweight='bold', pad=15)\n",
    "\n",
    "if len(np.unique(labels_pca_sample)) <= 10:\n",
    "    ax4.legend(loc='best', fontsize=8, ncol=2)\n",
    "\n",
    "ax4.grid(True, alpha=0.3, linestyle='--')\n",
    "ax4.tick_params(labelsize=10)\n",
    "\n",
    "var_explained = pca.explained_variance_ratio_\n",
    "ax4.text(\n",
    "    0.02, 0.98,\n",
    "    f\"Explained var: PC1={var_explained[0]:.1%}, PC2={var_explained[1]:.1%}\",\n",
    "    transform=ax4.transAxes,\n",
    "    fontsize=9,\n",
    "    verticalalignment='top',\n",
    "    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    ")\n",
    "\n",
    "plt.suptitle('KMeans Model Selection and Cluster Visualization',\n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.savefig(os.path.join(OUT_DIR, 'kmeans_model_selection_dashboard.png'),\n",
    "            dpi=200, bbox_inches='tight', facecolor='white')\n",
    "plt.close()\n",
    "\n",
    "print(f\"[OK] Model selection dashboard saved to {OUT_DIR}/kmeans_model_selection_dashboard.png\")\n",
    "\n",
    "metrics_df.to_csv(os.path.join(OUT_DIR, \"kmeans_metrics_sweep_full.csv\"), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
