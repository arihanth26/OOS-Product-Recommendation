{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d589d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bucket features from: artifacts/features/X_bucket_pca.npy\n",
      "Loading bucket index mapping from: artifacts/features/bucket_index.parquet\n",
      "Loading P0->P1 mapping from: artifacts/buckets/product_to_bucket.parquet\n",
      "Loading P0->P1 mapping from: artifacts/buckets/product_to_bucket.parquet\n",
      "\n",
      "Successfully calculated and saved 35429 rows of closest neighbors to ../data/processed/product_substitution_neighbors_with_p0.csv\n",
      "**NOTE:** This table lists Source P0 product IDs joined to the Source P1 bucket. Neighbor P1 buckets are provided as P1 IDs (Closest_1_P1_ID ... Closest_10_P1_ID).\n",
      "\n",
      "Result Head:\n",
      "   Source_Product_ID_P0  Source_P1_Bucket_ID  GMM_Cluster_ID  Closest_1_P1_ID  \\\n",
      "0                  2343                  844               0            26851   \n",
      "1                 10917                  945               0            12856   \n",
      "2                 49680                  986               0            31886   \n",
      "3                 30699                  992               0            21126   \n",
      "4                 41532                  993               0             6328   \n",
      "\n",
      "   Distance_1  Closest_2_P1_ID  Distance_2  Closest_3_P1_ID  Distance_3  \\\n",
      "0    0.037477             4781    0.043428            34150    0.046686   \n",
      "1    0.246748            13732    0.251234             7593    0.281849   \n",
      "2    0.268671            28228    0.274905             6525    0.388281   \n",
      "3    0.127962            19955    0.129530            16387    0.146884   \n",
      "4    0.107830            14731    0.147638            20890    0.180733   \n",
      "\n",
      "   Closest_4_P1_ID  ...  Closest_6_P1_ID  Distance_6  Closest_7_P1_ID  \\\n",
      "0             2212  ...             2197    0.100203            19956   \n",
      "1            23378  ...            10647    1.249410            16370   \n",
      "2            26348  ...             1742    1.177387            14774   \n",
      "3             2194  ...            14063    0.150404            27434   \n",
      "4            31774  ...             3313    1.050989            22924   \n",
      "\n",
      "   Distance_7  Closest_8_P1_ID  Distance_8  Closest_9_P1_ID  Distance_9  \\\n",
      "0    0.128460            22418    0.168683            21829    0.222634   \n",
      "1    1.338341             4326    1.348863            18370    1.383918   \n",
      "2    1.196376            18370    1.197076            10647    1.232138   \n",
      "3    0.150423            16388    0.151096            18043    0.158974   \n",
      "4    1.404736            23456    1.406959            33556    1.407229   \n",
      "\n",
      "   Closest_10_P1_ID  Distance_10  \n",
      "0             19411     0.362063  \n",
      "1             32136     1.385591  \n",
      "2              3313     1.296778  \n",
      "3             18042     0.168730  \n",
      "4              7837     1.409434  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "Successfully calculated and saved 35429 rows of closest neighbors to ../data/processed/product_substitution_neighbors_with_p0.csv\n",
      "**NOTE:** This table lists Source P0 product IDs joined to the Source P1 bucket. Neighbor P1 buckets are provided as P1 IDs (Closest_1_P1_ID ... Closest_10_P1_ID).\n",
      "\n",
      "Result Head:\n",
      "   Source_Product_ID_P0  Source_P1_Bucket_ID  GMM_Cluster_ID  Closest_1_P1_ID  \\\n",
      "0                  2343                  844               0            26851   \n",
      "1                 10917                  945               0            12856   \n",
      "2                 49680                  986               0            31886   \n",
      "3                 30699                  992               0            21126   \n",
      "4                 41532                  993               0             6328   \n",
      "\n",
      "   Distance_1  Closest_2_P1_ID  Distance_2  Closest_3_P1_ID  Distance_3  \\\n",
      "0    0.037477             4781    0.043428            34150    0.046686   \n",
      "1    0.246748            13732    0.251234             7593    0.281849   \n",
      "2    0.268671            28228    0.274905             6525    0.388281   \n",
      "3    0.127962            19955    0.129530            16387    0.146884   \n",
      "4    0.107830            14731    0.147638            20890    0.180733   \n",
      "\n",
      "   Closest_4_P1_ID  ...  Closest_6_P1_ID  Distance_6  Closest_7_P1_ID  \\\n",
      "0             2212  ...             2197    0.100203            19956   \n",
      "1            23378  ...            10647    1.249410            16370   \n",
      "2            26348  ...             1742    1.177387            14774   \n",
      "3             2194  ...            14063    0.150404            27434   \n",
      "4            31774  ...             3313    1.050989            22924   \n",
      "\n",
      "   Distance_7  Closest_8_P1_ID  Distance_8  Closest_9_P1_ID  Distance_9  \\\n",
      "0    0.128460            22418    0.168683            21829    0.222634   \n",
      "1    1.338341             4326    1.348863            18370    1.383918   \n",
      "2    1.196376            18370    1.197076            10647    1.232138   \n",
      "3    0.150423            16388    0.151096            18043    0.158974   \n",
      "4    1.404736            23456    1.406959            33556    1.407229   \n",
      "\n",
      "   Closest_10_P1_ID  Distance_10  \n",
      "0             19411     0.362063  \n",
      "1             32136     1.385591  \n",
      "2              3313     1.296778  \n",
      "3             18042     0.168730  \n",
      "4              7837     1.409434  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# The GMM k-NN function remains the same (keeps feature column prefix 'feature_').\n",
    "def find_k_nearest_neighbors_within_cluster(df_features_and_clusters, k=10):\n",
    "    \"\"\"\n",
    "    Finds the k closest products (P1 buckets) to every product within their assigned GMM cluster (P2).\n",
    "    Expects df_features_and_clusters to contain 'P1_Bucket_ID', 'GMM_Cluster_ID' and feature_* columns.\n",
    "    \"\"\"\n",
    "    feature_cols = [col for col in df_features_and_clusters.columns if col.startswith('feature_')]\n",
    "\n",
    "    if not feature_cols:\n",
    "        raise ValueError(\"Feature columns (e.g., 'feature_1' to 'feature_96') not found.\")\n",
    "\n",
    "    all_results = []\n",
    "    for cluster_id, cluster_data in df_features_and_clusters.groupby('GMM_Cluster_ID'):\n",
    "        X = cluster_data[feature_cols].values\n",
    "        p1_ids = cluster_data['P1_Bucket_ID'].values\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        if n_samples <= k:\n",
    "            # skip clusters that don't have enough members\n",
    "            continue\n",
    "\n",
    "        nn_model = NearestNeighbors(n_neighbors=k + 1, metric='euclidean', algorithm='auto')\n",
    "        nn_model.fit(X)\n",
    "        distances, indices = nn_model.kneighbors(X, return_distance=True)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            source_p1 = p1_ids[i]\n",
    "            neighbor_indices = indices[i, 1:]  # skip self\n",
    "            neighbor_distances = distances[i, 1:]\n",
    "            closest_p1_ids = p1_ids[neighbor_indices]\n",
    "\n",
    "            result = {'Source_P1_Bucket_ID': source_p1, 'GMM_Cluster_ID': cluster_id}\n",
    "            for j in range(k):\n",
    "                result[f'Closest_{j+1}_P1_ID'] = closest_p1_ids[j]\n",
    "                result[f'Distance_{j+1}'] = float(neighbor_distances[j])\n",
    "            all_results.append(result)\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# --- Example Usage: load real artifacts when available ---\n",
    "try:\n",
    "    # 1. Load Cluster Data (P1/P2)\n",
    "    cluster_csv_candidates = [\"../data/processed/gmm_cluster_product_details_for_analysis.csv\", \"data/processed/gmm_cluster_product_details_for_analysis.csv\", \"gmm_cluster_product_details_for_analysis.csv\"]\n",
    "    cluster_path = None\n",
    "    for p in cluster_csv_candidates:\n",
    "        if os.path.exists(p):\n",
    "            cluster_path = p\n",
    "            break\n",
    "    if cluster_path is None:\n",
    "        raise FileNotFoundError(\"Could not find gmm_cluster_product_details_for_analysis.csv in expected locations.\")\n",
    "    cluster_df = pd.read_csv(cluster_path)\n",
    "    cluster_df = cluster_df[['P1_Bucket_ID', 'GMM_Cluster_ID']].drop_duplicates()\n",
    "\n",
    "    # 2. Load bucket-level features (prefer X_bucket_pca.npy) and bucket index mapping\n",
    "    feature_npy_candidates = [\"artifacts/features/X_bucket_pca.npy\", \"artifacts/features/X_product_pca.npy\", \"../notebooks/artifacts/features/X_bucket_pca.npy\", \"notebooks/artifacts/features/X_bucket_pca.npy\"]\n",
    "    bucket_index_candidates = [\"artifacts/features/bucket_index.parquet\", \"artifacts/features/bucket_index.csv\", \"artifacts/buckets/bucket_index.parquet\", \"notebooks/artifacts/features/bucket_index.parquet\"]\n",
    "\n",
    "    feature_npy_path = None\n",
    "    for p in feature_npy_candidates:\n",
    "        if os.path.exists(p):\n",
    "            feature_npy_path = p\n",
    "            break\n",
    "\n",
    "    bucket_index_path = None\n",
    "    for p in bucket_index_candidates:\n",
    "        if os.path.exists(p):\n",
    "            bucket_index_path = p\n",
    "            break\n",
    "\n",
    "    if feature_npy_path is None or bucket_index_path is None:\n",
    "        # fallback: try to load precomputed notebook artifacts relative to this notebook\n",
    "        alt_feature = 'artifacts/features/X_bucket_pca.npy'\n",
    "        alt_index = 'artifacts/features/bucket_index.parquet'\n",
    "        if os.path.exists(alt_feature) and os.path.exists(alt_index):\n",
    "            feature_npy_path = alt_feature\n",
    "            bucket_index_path = alt_index\n",
    "\n",
    "    if feature_npy_path is None or bucket_index_path is None:\n",
    "        raise FileNotFoundError(\"Could not find bucket features (X_bucket_pca.npy) or bucket_index.parquet.\")\n",
    "\n",
    "    print(f\"Loading bucket features from: {feature_npy_path}\")\n",
    "    X_buckets = np.load(feature_npy_path)\n",
    "\n",
    "    print(f\"Loading bucket index mapping from: {bucket_index_path}\")\n",
    "    # bucket_index is expected to have a column linking each row of X_buckets to a P1_Bucket_ID\n",
    "    if bucket_index_path.endswith('.parquet'):\n",
    "        bucket_index = pd.read_parquet(bucket_index_path)\n",
    "    else:\n",
    "        bucket_index = pd.read_csv(bucket_index_path)\n",
    "\n",
    "    # Try to find P1 id column in bucket_index\n",
    "    if 'P1_Bucket_ID' in bucket_index.columns:\n",
    "        p1_col = 'P1_Bucket_ID'\n",
    "    elif 'P1' in bucket_index.columns:\n",
    "        p1_col = 'P1'\n",
    "    else:\n",
    "        # try index\n",
    "        if bucket_index.index.is_unique:\n",
    "            bucket_index = bucket_index.reset_index()\n",
    "            p1_col = bucket_index.columns[0]\n",
    "        else:\n",
    "            raise ValueError(\"Could not infer P1_Bucket_ID column from bucket_index file.\")\n",
    "\n",
    "    p1_list = bucket_index[p1_col].values\n",
    "    if len(p1_list) != X_buckets.shape[0]:\n",
    "        raise ValueError(\"Length mismatch: bucket features rows do not match bucket_index rows.\")\n",
    "\n",
    "    # Create feature dataframe\n",
    "    feature_cols = [f'feature_{i}' for i in range(1, X_buckets.shape[1] + 1)]\n",
    "    df_features = pd.DataFrame(X_buckets, columns=feature_cols)\n",
    "    df_features['P1_Bucket_ID'] = p1_list\n",
    "\n",
    "    # 3. Create the required input for the k-NN function\n",
    "    df_features_and_clusters = pd.merge(cluster_df, df_features, on='P1_Bucket_ID', how='inner')\n",
    "\n",
    "    # 4. Run the k-NN calculation (P1 to P1)\n",
    "    df_closest_neighbors = find_k_nearest_neighbors_within_cluster(df_features_and_clusters, k=10)\n",
    "\n",
    "    # 5. Load / infer P0 -> P1 mapping (try several artifact locations)\n",
    "    p0p1_candidates = [\n",
    "        'artifacts/buckets/product_to_bucket.parquet',\n",
    "        'artifacts/features/meta_product.parquet',\n",
    "        'artifacts/graph/nodes_p1_buckets.csv',\n",
    "        '../notebooks/artifacts/buckets/product_to_bucket.parquet',\n",
    "        '../notebooks/artifacts/features/meta_product.parquet',\n",
    "        '../data/processed/products_with_prices_ingredients_nutrition.csv'\n",
    "    ]\n",
    "    p0p1_path = None\n",
    "    for p in p0p1_candidates:\n",
    "        if os.path.exists(p):\n",
    "            p0p1_path = p\n",
    "            break\n",
    "\n",
    "    if p0p1_path is None:\n",
    "        # as a last resort, try to build a non-redundant mapping from the products CSV if present\n",
    "        raise FileNotFoundError(\"Could not find a P0->P1 mapping file. Looked for product_to_bucket.parquet or meta_product.parquet.\")\n",
    "\n",
    "    print(f\"Loading P0->P1 mapping from: {p0p1_path}\")\n",
    "    if p0p1_path.endswith('.parquet'):\n",
    "        df_map = pd.read_parquet(p0p1_path)\n",
    "    else:\n",
    "        df_map = pd.read_csv(p0p1_path)\n",
    "\n",
    "    # Normalize columns to 'product_id' and 'P1_Bucket_ID' if possible\n",
    "    possible_pid = [c for c in df_map.columns if 'product' in c.lower()][:1]\n",
    "    possible_p1 = [c for c in df_map.columns if 'p1' in c.lower() or 'bucket' in c.lower()][:1]\n",
    "    if not possible_pid or not possible_p1:\n",
    "        # try common names\n",
    "        if 'product_id' in df_map.columns and 'P1_Bucket_ID' in df_map.columns:\n",
    "            df_p0_p1_map = df_map[['product_id', 'P1_Bucket_ID']].copy()\n",
    "        else:\n",
    "            raise ValueError(\"Could not find product_id and P1 mapping columns in the P0->P1 file.\")\n",
    "    else:\n",
    "        df_p0_p1_map = df_map[[possible_pid[0], possible_p1[0]]].copy()\n",
    "        df_p0_p1_map.columns = ['product_id', 'P1_Bucket_ID']\n",
    "\n",
    "    # 6. Join the P0 IDs to the k-NN results\n",
    "    df_final = pd.merge(df_closest_neighbors, df_p0_p1_map, left_on='Source_P1_Bucket_ID', right_on='P1_Bucket_ID', how='left')\n",
    "    df_final = df_final.rename(columns={'product_id': 'Source_Product_ID_P0'})\n",
    "    df_final = df_final.drop(columns=['P1_Bucket_ID'])\n",
    "\n",
    "    # Reorder columns to put the new P0 ID up front\n",
    "    cols = ['Source_Product_ID_P0', 'Source_P1_Bucket_ID', 'GMM_Cluster_ID'] + [col for col in df_final.columns if col not in ['Source_Product_ID_P0', 'Source_P1_Bucket_ID', 'GMM_Cluster_ID']]\n",
    "    df_final = df_final[cols]\n",
    "\n",
    "    # 7. Output the results to CSV\n",
    "    output_filename = '../data/processed/product_substitution_neighbors_with_p0.csv'\n",
    "    os.makedirs(os.path.dirname(output_filename), exist_ok=True)\n",
    "    df_final.to_csv(output_filename, index=False)\n",
    "\n",
    "    print(f\"\\nSuccessfully calculated and saved {len(df_final)} rows of closest neighbors to {output_filename}\")\n",
    "    print(f\"**NOTE:** This table lists Source P0 product IDs joined to the Source P1 bucket. Neighbor P1 buckets are provided as P1 IDs (Closest_1_P1_ID ... Closest_10_P1_ID).\")\n",
    "    print(\"\\nResult Head:\")\n",
    "    try:\n",
    "        print(df_final.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "    except Exception:\n",
    "        print(df_final.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\nAn error occurred during execution. Ensure bucket feature artifacts and the P0->P1 mapping file are available.\")\n",
    "    print(f\"Error details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c964f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
